{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7186d02",
   "metadata": {},
   "source": [
    "# Document Summarizer with Ollama (gemma:2b)\n",
    "\n",
    "Extract text from DOCX, PPTX, PDF, VTT, HTML, and TXT files. Optionally generate summaries using local Ollama (`gemma:2b`).\n",
    "\n",
    "**Workflow**: Place files in `./input/` ‚Üí Run extraction ‚Üí Outputs saved to `./docs/` ‚Üí Use with `llama_index_rag.ipynb` for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip3 install python-docx python-pptx PyPDF2 webvtt-py beautifulsoup4 python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b973016",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677beb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Document processing libraries\n",
    "import docx\n",
    "from pptx import Presentation\n",
    "import PyPDF2\n",
    "import webvtt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cb255",
   "metadata": {},
   "source": [
    "## Ollama LLM Configuration\n",
    "\n",
    "Ensure Ollama is running at `http://localhost:11434` with `gemma:2b` available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Ollama caller using requests\n",
    "import requests\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "OLLAMA_BASE = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma:2b')\n",
    "OLLAMA_TIMEOUT = int(os.getenv('OLLAMA_TIMEOUT', '120'))\n",
    "\n",
    "def call_llm(messages: List[Dict[str, str]], model: str | None = None, temperature: float = 0.1, timeout: int | None = None) -> Dict[str, Any]:\n",
    "    \"\"\"Call local Ollama chat endpoint and return parsed content.\"\"\"\n",
    "    model = model or OLLAMA_MODEL\n",
    "    timeout = timeout or OLLAMA_TIMEOUT\n",
    "\n",
    "    url = f\"{OLLAMA_BASE}/api/chat\"\n",
    "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature}\n",
    "    try:\n",
    "        resp = requests.post(url, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        # Ollama responses can vary; try common locations for text\n",
    "        content = ''\n",
    "        if isinstance(data, dict):\n",
    "            # standard OpenAI-like choices.response structure\n",
    "            if 'choices' in data and isinstance(data['choices'], list) and data['choices']:\n",
    "                choice = data['choices'][0]\n",
    "                if isinstance(choice, dict):\n",
    "                    # try nested message content\n",
    "                    msg = choice.get('message') or choice.get('delta') or choice.get('text')\n",
    "                    if isinstance(msg, dict):\n",
    "                        content = msg.get('content', '')\n",
    "                    elif isinstance(msg, str):\n",
    "                        content = msg\n",
    "            # fallback fields\n",
    "            if not content and 'text' in data:\n",
    "                content = data.get('text', '')\n",
    "            if not content:\n",
    "                content = str(data)\n",
    "        else:\n",
    "            content = str(data)\n",
    "\n",
    "        return {'content': content, 'raw': data}\n",
    "    except Exception as e:\n",
    "        return {'content': '', 'error': str(e)}\n",
    "\n",
    "# Quick smoke test (uncomment to run)\n",
    "# test = call_llm([{'role':'system','content':'You are a helpful assistant.'},{'role':'user','content':'Say hello in one sentence.'}])\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test call_llm interactively (uncomment to run)\n",
    "# test = call_llm([{'role':'user','content':'Say hello in one sentence.'}])\n",
    "# print(test.get('content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68212c",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text from DOCX file\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text_content = []\n",
    "        \n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                text_content.append(paragraph.text)\n",
    "        \n",
    "        # Extract text from tables\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        text_content.append(cell.text)\n",
    "        \n",
    "        return \"\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting DOCX: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_pptx(file_path: str) -> str:\n",
    "    \"\"\"Extract text from PowerPoint PPTX file\"\"\"\n",
    "    try:\n",
    "        prs = Presentation(file_path)\n",
    "        text_content = []\n",
    "        \n",
    "        for slide_num, slide in enumerate(prs.slides, 1):\n",
    "            text_content.append(f\"\\n--- Slide {slide_num} ---\")\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                    text_content.append(shape.text)\n",
    "        \n",
    "        return \"\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting PPTX: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    try:\n",
    "        text_content = []\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                if text.strip():\n",
    "                    text_content.append(f\"\\n--- Page {page_num + 1} ---\")\n",
    "                    text_content.append(text)\n",
    "        \n",
    "        return \"\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting PDF: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_vtt(file_path: str) -> str:\n",
    "    \"\"\"Extract text from WebVTT subtitle file\"\"\"\n",
    "    try:\n",
    "        text_content = []\n",
    "        \n",
    "        for caption in webvtt.read(file_path):\n",
    "            text = caption.text.strip()\n",
    "            if text:\n",
    "                text_content.append(text)\n",
    "        \n",
    "        return \" \".join(text_content)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting VTT: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_html(file_path: str) -> str:\n",
    "    \"\"\"Extract text from HTML file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting HTML: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_txt(file_path: str) -> str:\n",
    "    \"\"\"Extract text from plain text file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting TXT: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Text extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f7ae0",
   "metadata": {},
   "source": [
    "## Universal Document Processor\n",
    "\n",
    "Automatically detects file type and extracts text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_document(file_path: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Universal document text extractor\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the document\n",
    "        \n",
    "    Returns:\n",
    "        Dict with text, file_type, and success status\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return {\n",
    "            'text': '',\n",
    "            'file_type': 'unknown',\n",
    "            'success': False,\n",
    "            'error': f\"File not found: {file_path}\"\n",
    "        }\n",
    "    \n",
    "    # Map file extensions to extraction functions\n",
    "    extractors = {\n",
    "        '.docx': extract_text_from_docx,\n",
    "        '.pptx': extract_text_from_pptx,\n",
    "        '.pdf': extract_text_from_pdf,\n",
    "        '.vtt': extract_text_from_vtt,\n",
    "        '.html': extract_text_from_html,\n",
    "        '.htm': extract_text_from_html,\n",
    "        '.txt': extract_text_from_txt,\n",
    "    }\n",
    "    \n",
    "    file_ext = file_path.suffix.lower()\n",
    "    \n",
    "    if file_ext not in extractors:\n",
    "        return {\n",
    "            'text': '',\n",
    "            'file_type': file_ext,\n",
    "            'success': False,\n",
    "            'error': f\"Unsupported file type: {file_ext}\"\n",
    "        }\n",
    "    \n",
    "    print(f\"üìÑ Processing {file_path.name} ({file_ext})...\")\n",
    "    \n",
    "    text = extractors[file_ext](str(file_path))\n",
    "    \n",
    "    if text.startswith(\"Error\"):\n",
    "        return {\n",
    "            'text': '',\n",
    "            'file_type': file_ext,\n",
    "            'success': False,\n",
    "            'error': text\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'file_type': file_ext,\n",
    "        'success': True,\n",
    "        'char_count': len(text),\n",
    "        'word_count': len(text.split())\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Universal document processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b713eed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c61a005",
   "metadata": {},
   "source": [
    "## AI Summarization with Ollama (gemma:2b)\n",
    "\n",
    "Generate concise summaries using the local Ollama model (`gemma:2b`). Ensure Ollama is running before calling these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2997b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_with_ai(text: str, max_words: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Summarize text using Ollama (gemma:2b)\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        max_words: Maximum words in summary\n",
    "        \n",
    "    Returns:\n",
    "        Summary text\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) < 100:\n",
    "        return \"Text too short to summarize.\"\n",
    "    \n",
    "    # Truncate very long documents\n",
    "    if len(text) > 50000:\n",
    "        print(f\"  ‚ö†Ô∏è Text is {len(text)} chars, truncating to 50000...\")\n",
    "        text = text[:50000] + \"\\n\\n[Document truncated for summarization]\"\n",
    "    \n",
    "    prompt = f\"\"\"Please provide a comprehensive summary of the following document. \n",
    "The summary should:\n",
    "- Be around {max_words} words\n",
    "- Capture the main topics and key points\n",
    "- Be well-structured and easy to read\n",
    "- Include important details and facts\n",
    "\n",
    "Document:\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        response = call_llm(messages)\n",
    "        summary = response.get('content', '').strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "\n",
    "def summarize_with_chunks(text: str, chunk_size: int = 10000, max_words: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Summarize very long documents by chunking\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        chunk_size: Size of each chunk\n",
    "        max_words: Maximum words in final summary\n",
    "        \n",
    "    Returns:\n",
    "        Summary text\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return summarize_text_with_ai(text, max_words)\n",
    "    \n",
    "    print(f\"  üìö Document is {len(text)} chars, using chunked summarization...\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    \n",
    "    print(f\"  üìÑ Processing {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"    Chunk {i}/{len(chunks)}...\", end=\" \")\n",
    "        summary = summarize_text_with_ai(chunk, max_words // len(chunks))\n",
    "        chunk_summaries.append(summary)\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    # Combine chunk summaries\n",
    "    combined = \"\\n\\n\".join(chunk_summaries)\n",
    "    \n",
    "    # Final summary of summaries\n",
    "    print(f\"  üîÑ Generating final summary...\")\n",
    "    final_summary = summarize_text_with_ai(combined, max_words)\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "\n",
    "print(\"‚úÖ AI summarization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaffb8",
   "metadata": {},
   "source": [
    "## Complete Processing Pipeline\n",
    "\n",
    "Process documents and save as text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(\n",
    "    input_path: str = \"./input\",\n",
    "    output_dir: str = \"./docs\",\n",
    "    summarize: bool = False,\n",
    "    max_summary_words: int = 500\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete document processing pipeline\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input document\n",
    "        output_dir: Directory to save output (default: ./docs for RAG)\n",
    "        summarize: Whether to generate AI summary\n",
    "        max_summary_words: Maximum words in summary\n",
    "        \n",
    "    Returns:\n",
    "        Dict with processing results\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {input_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extract text\n",
    "    result = extract_text_from_document(input_path)\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(f\"‚ùå {result['error']}\")\n",
    "        return result\n",
    "    \n",
    "    text = result['text']\n",
    "    print(f\"‚úÖ Extracted {result['char_count']:,} characters ({result['word_count']:,} words)\")\n",
    "    \n",
    "    # Save extracted text\n",
    "    text_output = output_dir / f\"{input_path.stem}_extracted.txt\"\n",
    "    with open(text_output, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    print(f\"‚úÖ Saved extracted text: {text_output.name}\")\n",
    "    \n",
    "    # Generate summary if requested\n",
    "    if summarize:\n",
    "        print(f\"\\nü§ñ Generating AI summary...\")\n",
    "        summary = summarize_with_chunks(text, max_words=max_summary_words)\n",
    "        \n",
    "        if not summary.startswith(\"Error\"):\n",
    "            summary_output = output_dir / f\"{input_path.stem}_summary.txt\"\n",
    "            with open(summary_output, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"# Summary of {input_path.name}\\n\\n\")\n",
    "                f.write(summary)\n",
    "            print(f\"‚úÖ Saved summary: {summary_output.name}\")\n",
    "            print(f\"   Summary length: {len(summary.split())} words\")\n",
    "            \n",
    "            result['summary'] = summary\n",
    "            result['summary_file'] = str(summary_output)\n",
    "        else:\n",
    "            print(f\"‚ùå {summary}\")\n",
    "            result['summary'] = None\n",
    "    \n",
    "    result['text_file'] = str(text_output)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ Document processing pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef42ac",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process entire folders of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c171df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(\n",
    "    input_folder: str,\n",
    "    output_dir: str = \"./docs\",\n",
    "    summarize: bool = False,\n",
    "    file_extensions: List[str] = None\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Process all documents in a folder\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder with documents\n",
    "        output_dir: Directory to save outputs (default: ./docs for RAG)\n",
    "        summarize: Whether to generate AI summaries\n",
    "        file_extensions: List of extensions to process\n",
    "        \n",
    "    Returns:\n",
    "        Dict with batch processing results\n",
    "    \"\"\"\n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.docx', '.pptx', '.pdf', '.vtt', '.html', '.htm', '.txt']\n",
    "    \n",
    "    input_folder = Path(input_folder)\n",
    "    \n",
    "    if not input_folder.exists():\n",
    "        print(f\"‚ùå Folder not found: {input_folder}\")\n",
    "        return {'success': False, 'error': 'Folder not found'}\n",
    "    \n",
    "    # Find all matching files\n",
    "    files = []\n",
    "    for ext in file_extensions:\n",
    "        files.extend(input_folder.glob(f\"*{ext}\"))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"‚ùå No matching files found in {input_folder}\")\n",
    "        return {'success': False, 'error': 'No files found'}\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BATCH PROCESSING: {len(files)} files\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, file_path in enumerate(files, 1):\n",
    "        print(f\"\\n[{i}/{len(files)}] Processing {file_path.name}...\")\n",
    "        \n",
    "        try:\n",
    "            result = process_document(\n",
    "                str(file_path),\n",
    "                output_dir=output_dir,\n",
    "                summarize=summarize\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                \n",
    "            results.append({\n",
    "                'file': file_path.name,\n",
    "                'result': result\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file_path.name}: {str(e)}\")\n",
    "            failed += 1\n",
    "            results.append({\n",
    "                'file': file_path.name,\n",
    "                'result': {'success': False, 'error': str(e)}\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚úÖ Successful: {successful}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'total': len(files),\n",
    "        'successful': successful,\n",
    "        'failed': failed,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54ee5a",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start: Process Documents for RAG\n",
    "\n",
    "**Automated workflow to prepare documents for RAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76936776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Quick Start: Process all documents from input folder\n",
    "input_folder = \"./input\"\n",
    "output_folder = \"./docs\"  # Extracted text saved here for RAG\n",
    "\n",
    "# Create folders\n",
    "Path(input_folder).mkdir(exist_ok=True)\n",
    "Path(output_folder).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Input folder: {input_folder}\")\n",
    "print(f\"üìÇ Output folder: {output_folder}\")\n",
    "print(f\"\\nüí° Place your documents (docx, pptx, pdf, vtt, html, txt) in '{input_folder}'\")\n",
    "print(f\"   Extracted text will be saved to '{output_folder}' for RAG indexing\\n\")\n",
    "\n",
    "# Check for documents\n",
    "supported_extensions = ['.docx', '.pptx', '.pdf', '.vtt', '.txt', '.html', '.htm']\n",
    "all_files = []\n",
    "for ext in supported_extensions:\n",
    "    all_files.extend(Path(input_folder).glob(f\"*{ext}\"))\n",
    "\n",
    "if all_files:\n",
    "    print(f\"‚úÖ Found {len(all_files)} document(s) to process\\n\")\n",
    "    \n",
    "    # Process all documents\n",
    "    batch_result = process_folder(\n",
    "        input_folder=input_folder,\n",
    "        output_dir=output_folder,\n",
    "        summarize=True,  # Set to True if you want AI summaries\n",
    "        file_extensions=supported_extensions\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if batch_result['success']:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìä Total files: {batch_result['total']}\")\n",
    "        print(f\"‚úÖ Successful: {batch_result['successful']}\")\n",
    "        print(f\"‚ùå Failed: {batch_result['failed']}\")\n",
    "        print(f\"\\nüìÅ Extracted text saved to: {output_folder}/\")\n",
    "        print(f\"üí° Next: Run llama_index_rag.ipynb to index these documents\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No documents found in '{input_folder}'\")\n",
    "    print(f\"   Supported formats: {', '.join(supported_extensions)}\")\n",
    "    print(f\"\\nüí° Add documents to '{input_folder}' and run this cell again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b753e",
   "metadata": {},
   "source": [
    "## Example: Process Single Document\n",
    "\n",
    "Process and optionally summarize a single document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process a single document with AI summary\n",
    "example_file = \"./input/sample.docx\"  # Change to your file\n",
    "\n",
    "if Path(example_file).exists():\n",
    "    result = process_document(\n",
    "        input_path=example_file,\n",
    "        output_dir=\"./docs\",\n",
    "        summarize=True,  # Generate AI summary\n",
    "        max_summary_words=500\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\n‚úÖ Processing complete!\")\n",
    "        print(f\"   Extracted text: {result['text_file']}\")\n",
    "        if 'summary_file' in result:\n",
    "            print(f\"   Summary: {result['summary_file']}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Example file not found: {example_file}\")\n",
    "    print(\"Update the path to an actual document file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

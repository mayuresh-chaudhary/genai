{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245de6d3",
   "metadata": {},
   "source": [
    "# LlamaIndex RAG with Ollama (gemma:2b)\n",
    "\n",
    "Build a Retrieval-Augmented Generation (RAG) pipeline using local components: LlamaIndex for indexing, Ollama (`gemma:2b`) for LLM inference, and Weaviate as the vector store.\n",
    "\n",
    "For detailed setup, architecture, and troubleshooting, see `CHAPTER3_BLOG.md` in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08d2e9",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+ with virtual environment\n",
    "- Ollama at `http://localhost:11434` with `gemma:2b` available\n",
    "- Weaviate at `http://localhost:8080` (run: `docker run -d -p 8080:8080 -p 50051:50051 semitechnologies/weaviate:latest`)\n",
    "\n",
    "Check health:\n",
    "```bash\n",
    "curl http://localhost:11434/api/show\n",
    "curl http://localhost:8080/v1/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f786d",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "1. Ensure Ollama and Weaviate are running.\n",
    "2. Place documents in `./docs/` (or run the document summarizer to extract them).\n",
    "3. Run notebook cells in order: imports â†’ LLM config â†’ Weaviate connection â†’ load docs â†’ index â†’ query.\n",
    "4. Use `ask_question(...)` to run queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4307b",
   "metadata": {},
   "source": [
    "## Troubleshooting & Tips\n",
    "\n",
    "- ReadTimeouts: increase `keep_alive` and the HTTP timeout in the Ollama client; add retries/backoff around `query_engine.query` calls.\n",
    "- Slow embedding/indexing: use smaller batch sizes or a faster embedding model during development.\n",
    "- Weaviate schema issues: persist schema and avoid re-creating classes on every run; inspect Weaviate logs for errors.\n",
    "- Logs: enable verbose prints around the LLM call and Weaviate requests to capture failing payloads.\n",
    "\n",
    "If you'd like, I can add automatic retry logic for the Ollama calls and incorporate an exponential backoff helper in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip3 install llama-index llama-index-llms-azure-openai llama-index-embeddings-ollama llama-index-vector-stores-weaviate llama-index-llms-ollama weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5456c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "\n",
    "# Weaviate client\n",
    "import weaviate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure Ollama LLM for LlamaIndex\n",
    "llm = Ollama(\n",
    "    model=\"gemma:2b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    keep_alive=120,\n",
    "    # optional: pass other ollama-specific kwargs if needed:\n",
    "    # ollama_kwargs={\"n_predict\": 1024}\n",
    ")\n",
    "\n",
    "print(\"Ollama AI LLM configured\")\n",
    "\n",
    "# Configure Ollama embeddings\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "print(\"Ollama embeddings configured\")\n",
    "\n",
    "# Set global defaults for LlamaIndex\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"\\nLlamaIndex settings configured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate vector store\n",
    "print(\"ðŸ”Œ Connecting to Weaviate...\")\n",
    "\n",
    "connection_attempts = [\n",
    "    {\"host\": \"localhost\", \"port\": 8080, \"grpc_port\": 50051},\n",
    "    {\"host\": \"localhost\", \"port\": 8081, \"grpc_port\": 50051},\n",
    "    {\"host\": \"127.0.0.1\", \"port\": 8080, \"grpc_port\": 50051},\n",
    "]\n",
    "\n",
    "weaviate_client = None\n",
    "for config in connection_attempts:\n",
    "    try:\n",
    "        print(f\"  Trying {config['host']}:{config['port']}...\")\n",
    "        weaviate_client = weaviate.connect_to_local(\n",
    "            host=config['host'],\n",
    "            port=config['port'],\n",
    "            grpc_port=config['grpc_port']\n",
    "        )\n",
    "        print(f\"Connected to Weaviate at {config['host']}:{config['port']}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {str(e)[:50]}\")\n",
    "        continue\n",
    "\n",
    "if weaviate_client is None:\n",
    "    raise ConnectionError(\"Could not connect to Weaviate. Please start it with: podman run -d -p 8080:8080 -p 50051:50051 semitechnologies/weaviate:latest\")\n",
    "\n",
    "# Create Weaviate vector store for LlamaIndex\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=weaviate_client,\n",
    "    index_name=\"EcommerceDocuments\",\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Weaviate vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and create index\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "print(\"ðŸ“š Loading documents from docs folder...\")\n",
    "\n",
    "# Path to docs folder\n",
    "docs_folder = Path(\"./docs\")\n",
    "if not docs_folder.exists():\n",
    "    print(f\"older not found: {docs_folder}\")\n",
    "    print(\"Please create a 'docs' folder and add your .txt files\")\n",
    "    raise FileNotFoundError(f\"docs folder not found at {docs_folder}\")\n",
    "\n",
    "# Check for text files\n",
    "txt_files = list(docs_folder.glob(\"*.txt\"))\n",
    "if not txt_files:\n",
    "    print(f\"No .txt files found in {docs_folder}\")\n",
    "    raise FileNotFoundError(\"No text files found in docs folder\")\n",
    "\n",
    "print(f\"Found {len(txt_files)} text file(s):\")\n",
    "for txt_file in txt_files:\n",
    "    print(f\"   - {txt_file.name}\")\n",
    "\n",
    "# Load documents directly from the docs folder using SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_dir=str(docs_folder)).load_data()\n",
    "print(f\"\\nLoaded {len(documents)} document(s)\")\n",
    "\n",
    "# Display document info\n",
    "total_chars = sum(len(doc.text) for doc in documents)\n",
    "print(f\"   Total characters: {total_chars:,}\")\n",
    "\n",
    "# Parse documents into nodes with custom chunk size\n",
    "print(\"\\nðŸ”¨ Parsing documents into chunks...\")\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f\"Created {len(nodes)} nodes\")\n",
    "\n",
    "# Create storage context with Weaviate\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create vector store index\n",
    "print(\"\\nCreating vector store index...\")\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\nIndex created successfully!\")\n",
    "print(f\"   Total nodes indexed: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ad5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine with custom settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "print(\"âš™ï¸ Configuring query engine...\")\n",
    "\n",
    "# Configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "# Configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",  # Use compact mode for concise responses\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    response_mode=\"compact\",\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "print(\"Query engine configured\")\n",
    "print(f\"   Retrieval mode: Top-3 similarity search\")\n",
    "print(f\"   Response mode: Compact\")\n",
    "print(f\"   LLM: Circuit AI (gpt-4.1)\")\n",
    "print(f\"   Embeddings: Ollama (mxbai-embed-large)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "import textwrap\n",
    "\n",
    "def ask_question(question, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question using LlamaIndex RAG pipeline\n",
    "    \n",
    "    Args:\n",
    "        question (str): Your question\n",
    "        show_sources (bool): Whether to display source nodes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Query the index\n",
    "    print(\"\\nSearching knowledge base...\")\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Display answer\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(\"=\" * 80)\n",
    "    wrapped_answer = textwrap.fill(str(response), width=80)\n",
    "    print(wrapped_answer)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display source nodes if requested\n",
    "    if show_sources and hasattr(response, 'source_nodes'):\n",
    "        print(\"\\nSource Nodes:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, node in enumerate(response.source_nodes, 1):\n",
    "            score = node.score if hasattr(node, 'score') else 'N/A'\n",
    "            print(f\"\\n  Source {i} (Relevance: {score}):\")\n",
    "            text_preview = node.text[:200] + \"...\" if len(node.text) > 200 else node.text\n",
    "            wrapped_text = textwrap.fill(text_preview, width=76, initial_indent='    ', subsequent_indent='    ')\n",
    "            print(wrapped_text)\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Test with sample questions\n",
    "print(\"\\nLlamaIndex RAG System Ready!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run first test query\n",
    "test_response = ask_question(\"Create Jira User Stories for Personalised Product Recommendation\", show_sources=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e01e95",
   "metadata": {},
   "source": [
    "## Interactive Query Interface\n",
    "\n",
    "Use the cell below to ask questions about the documents you indexed. The query engine retrieves relevant chunks from Weaviate and synthesizes answers using the local Ollama model (`gemma:2b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ef227",
   "metadata": {},
   "source": [
    "## Advanced: Chat Engine with Conversation History\n",
    "\n",
    "LlamaIndex supports conversational interactions with memory. Responses are generated by the local Ollama model (`gemma:2b`) and retrieval uses Weaviate. For faster iteration, consider using a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat engine for conversational interactions\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "response = chat_engine.chat(\"Pre Event Questions, Create User Stories\")\n",
    "print(\"Assistant:\", response.response)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Follow-up question (maintains context)\n",
    "response = chat_engine.chat(\"Also Provide Acceptance Criteria\")\n",
    "print(\"Assistant:\", response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
